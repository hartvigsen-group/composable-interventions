defaults:
  - edit: none  # Choose any of [memit, lora, ft]
  - compression: none  # Choose any of [wanda, sparsegpt, gptq, awq]
  - unlearn: none  # Choose any of [rmu]
  - _self_

model_name: meta-llama/Meta-Llama-3-8B
#meta-llama/Meta-Llama-3-8B
#meta-llama/Llama-2-7b-chat-hf
dtype: torch.bfloat16
device: 0
model_parallel: true
seed: 42
tag: "default"
interventions: []  # List of interventions, choose any number of [edit, compress, unlearn]

# Weights and Biases Settings
wandb: disabled # disabled or online
wandb_entity: "dri-ice"
wandb_project: "Composable_Interventions"

alg_name: FT # overwritten by edit config but needs to be here
edit_dataset: "zsre"
stats_dir: "/scratch/{USER}/stats"
max_length: 30
batch_size: 50

save: out/
save_model: null
eval_zero_shot: false
compress: false
method: none
sparsity_ratio: 0.0
wbits: 16
compression_dataset: c4
dataset: c4

number_of_edits: 50
edit_set: 1

# The max number of questions per QA set. Should be null for main
# results,but can be set lower for fast debugging~
qa_question_count_limit: null

# RMU configs which can be overwritten
rmu_max_num_batches: 0
rmu_layer_id: -1

load_ckpt: False
ckpt_path: /scratch/sux7mp/saved_models/checkpoint_20240519_123419.pth
save_ckpt: False
