number_of_edits: 50
edit_set: 1
alg_name: "LoRA"
model_name: meta-llama/Meta-Llama-3-8B-Instruct
model: meta-llama/Meta-Llama-3-8B-Instruct
# llama: meta-llama/Llama-2-7b-chat-hf
# llama-3: meta-llama/Meta-Llama-3-8B-Instruct
# pythia: EleutherAI/pythia-14m
edit_dataset: "counterfact"
stats_dir: "/scratch/sux7mp/stats"
device: 0
tag: "default"

lora_type: "adalora"
layers: []
num_steps: 70  #70
lr: 5e-4 # 5e-3
weight_decay: 0.01
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.1
norm_constraint: false
target_modules: ["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
model_parallel: true

max_length: 40
batch_size: 50

# Compression
seed: 0
nsamples: 128
sparsity_ratio: 0.3
sparsity_type: unstructured
prune_method: sparsegpt # wanda or sparsegpt
quant_method: autogptq
compression_dataset: c4
dataset: c4
percdamp: 0.01
wbits: 4
zero_point: True
groupsize: 128
sym: true
nearest: false
new_eval: false
act_order: false
true_sequential: false
static_groups: false
cache_dir: /scratch/sux7mp/llm_weights
use_variant: false
save: out/
save_model: null
eval_zero_shot: false

compress_first: False
edit: True
compress: False
method: prune # prune or quant

load_ckpt: False
ckpt_path: /scratch/sux7mp/saved_models/checkpoint_20231221_113020.pth

save_ckpt: False

edit_train: false