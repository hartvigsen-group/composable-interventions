{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import wandb\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull and Dedup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_columns = [\n",
    "    # Overall\n",
    "    \"tag\",\n",
    "    # \"seed\",\n",
    "    \"_timestamp\",\n",
    "\n",
    "    # Interventions\n",
    "    \"interventions\", \"edit\", \"unlearn\", \"compression\", \"model_name\",\n",
    "\n",
    "    # Editing\n",
    "    \"edit_set\", \n",
    "    \"edit_dataset\", \"number_of_edits\",\n",
    "\n",
    "    # Unlearning\n",
    "    \"rmu_layer_id\",\n",
    "\n",
    "    # Compression\n",
    "    \"wbits\", \"compression_dataset\", \"sparsity_ratio\",\n",
    "]\n",
    "evaluation_columns = [\n",
    "    \"qa_question_count_limit\",  # An artifical max number of questions to ask during evaluation. Should be none when not debugging.\n",
    "    \"mmlu accuracy\",            # The accuracy of the model on the MMLU dataset. This measures overall model utility. Llama-3 should be ~62%\n",
    "    \"wmdp_bio accuracy\",        # The accuracy of the model on the WMDP bio split. This is the unlearning target. Should be ~25% when RMU is applied.\n",
    "    \"wmdp_cyber accuracy\",      # The accuracy of the model on the WMDP cyber split. This is the unlearning target. Should be ~25% when RMU is applied.\n",
    "    \"PPL\",                      # TODO:\n",
    "    \"PPL edits\",                # Perplexity for the edits. Should be low when editing is applied.\n",
    "    \"PPl QA\",                   # Perplexity for the QA. Should be low when QA is applied.\n",
    "    \"Generalization\",           # TODO: \n",
    "    \"FLOPs\",                    # TODO: \n",
    "    \"Success recall\",           # TODO:\n",
    "    \"Generalization recall\",    # TODO:\n",
    "    \"Locality\",                 # TODO:\n",
    "    \"Average bits\",             # TODO:\n",
    "    \"Rewrite accuracy\",         # TODO:\n",
    "    \"PPl edits unmasked\",       # TODO:\n",
    "    \"Local recall\",             # TODO:\n",
    "    \"Latency\",                  # TODO:\n",
    "]\n",
    "relevant_columns = setting_columns + evaluation_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composable_Interventions has all the results\n",
    "project_paths = [\n",
    "    'dri-ice/Composable_Interventions',\n",
    "    # 'dri-ice/AK_Tests'\n",
    "]\n",
    "\n",
    "filter_dict = { \"state\": \"finished\" }\n",
    "data_frames = []\n",
    "for project_path in project_paths:\n",
    "    runs = api.runs(project_path, filters=filter_dict)\n",
    "    \n",
    "    # Iterate over eachrun and capture the c        onfig and summary metrics\n",
    "    for run in tqdm(runs, desc=project_path):\n",
    "        try:\n",
    "            run_start_datetime = datetime.fromtimestamp(run.summary_metrics[\"_timestamp\"])\n",
    "            start_cutoff = datetime.strptime(\"2024-05-18 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            end_cutoff = datetime.strptime(\"2024-06-19 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "            if run_start_datetime < start_cutoff or run_start_datetime > end_cutoff:\n",
    "                continue\n",
    "\n",
    "            skip_tags = [\"test\", \"hparam_search\"]\n",
    "            should_skip = False\n",
    "            for tag in skip_tags:\n",
    "                if tag in run.config[\"tag\"].lower():\n",
    "                    should_skip = True\n",
    "            \n",
    "            if should_skip:\n",
    "                continue\n",
    "\n",
    "            config_frame = pd.DataFrame([run.config])\n",
    "            summary_frame = pd.DataFrame([run.summary_metrics])\n",
    "            combined_frame = pd.concat([config_frame, summary_frame], axis=1)\n",
    "            data_frames.append(combined_frame)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing run {run.id}: {e}\")\n",
    "\n",
    "# Sort by 'tag' and '_timestamp' in descending order to have the most recent run first\n",
    "all_runs_df = pd.concat(data_frames, ignore_index=True)[relevant_columns]\n",
    "all_runs_df[\"interventions\"] = all_runs_df[\"interventions\"].astype(str)\n",
    "\n",
    "# Keep only the current edit dataset\n",
    "all_runs_df = all_runs_df[all_runs_df['edit_dataset'] == 'zsre']\n",
    "\n",
    "# WARNING: WHAT DOES EDIT SET 50 MEAN COMPARED TO EDIT SET 1?\n",
    "# all_runs_df = all_runs_df[all_runs_df[\"edit_set\"] == 50]\n",
    "# all_runs_df_sorted = all_runs_df.sort_values(by=['tag', '_timestamp'], ascending=[True, False])\n",
    "all_runs_df[\"date\"] = pd.to_datetime(all_runs_df[\"_timestamp\"], unit='s')\n",
    "all_runs_df_sorted = all_runs_df.sort_values(by=['_timestamp'], ascending=[False])\n",
    "all_runs_df_sorted = all_runs_df_sorted[all_runs_df_sorted[\"qa_question_count_limit\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the recency column, for example, 'date'\n",
    "all_runs_df_sorted = all_runs_df_sorted.sort_values(by='date')\n",
    "\n",
    "# Drop duplicates, keeping only the most recent occurrence for each \"tag\" and \"edit_set\"\n",
    "latest_runs_df = all_runs_df_sorted.drop_duplicates(subset=['tag', 'edit_set'], keep='last')\n",
    "\n",
    "# Define a function to calculate standard error\n",
    "def standard_error(x):\n",
    "    return x.std() / np.sqrt(len(x))\n",
    "\n",
    "# Group by the \"tag\" column and calculate the mean for numerical columns\n",
    "grouped_df = latest_runs_df.groupby('tag').agg(['mean', standard_error])\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "grouped_df.columns = [f'{col[0]}_{col[1]}' for col in grouped_df.columns]\n",
    "\n",
    "# Split the columns into means and standard errors\n",
    "mean_columns = [col for col in grouped_df.columns if col.endswith('_mean')]\n",
    "se_columns = [col for col in grouped_df.columns if col.endswith('_standard_error')]\n",
    "\n",
    "# Create separate DataFrames for means and standard errors\n",
    "mean_df = grouped_df[mean_columns].rename(columns=lambda x: x.replace('_mean', ''))\n",
    "se_df = grouped_df[se_columns].rename(columns=lambda x: x.replace('_standard_error', '_se'))\n",
    "\n",
    "# Merge the means and standard errors back into one DataFrame\n",
    "all_runs_df_sorted_averaged = pd.concat([mean_df, se_df], axis=1)\n",
    "\n",
    "# Reset index if needed\n",
    "all_runs_df_sorted_averaged.reset_index(inplace=True)\n",
    "\n",
    "# Add non-numerical columns from the latest_runs_df\n",
    "non_numerical_columns = latest_runs_df.select_dtypes(exclude=[np.number]).drop_duplicates(subset=['tag'])\n",
    "all_runs_df_sorted_averaged = all_runs_df_sorted_averaged.merge(non_numerical_columns, on='tag', how='left')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "all_runs_df_sorted_averaged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'edit_set' is either 1 or 50\n",
    "filtered_df = all_runs_df_sorted_averaged[all_runs_df_sorted_averaged['edit_set'].isin([1, 50])]\n",
    "\n",
    "# Print the unique 'tag' values\n",
    "unique_tags = filtered_df['tag'].unique()\n",
    "\n",
    "print([tag for tag in unique_tags if \"rmu\" in tag])\n",
    "print(unique_tags)\n",
    "print(all_runs_df_sorted_averaged.shape)\n",
    "print(all_runs_df_sorted.columns)\n",
    "print(all_runs_df_sorted_averaged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ensure that rmu_layer_id is 3. This was originaly set to 5, but decided to rerun the evals last minute with a better hyperparameter.\n",
    "# Use older RMU experiments\n",
    "# all_runs_df_sorted = all_runs_df_sorted[all_runs_df_sorted[\"rmu_layer_id\"] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates, keeping only the first occurrence (which is the most recent due to sorting)\n",
    "# all_runs_df_deduplicated = all_runs_df_sorted.drop_duplicates(subset=[col for col in setting_columns if col not in [\"_timestamp\", \"tag\", \"date\"]], keep=\"first\")\n",
    "all_runs_df_deduplicated = all_runs_df_sorted_averaged.drop_duplicates(subset=\"tag\", keep=\"first\")\n",
    "all_runs_df_deduplicated[\"interventions\"] = all_runs_df_deduplicated[\"interventions\"].apply(lambda x : ast.literal_eval(x))\n",
    "\n",
    "rename_dict = {\n",
    "    \"meta-llama/Meta-Llama-3-8B\" : \"Llama-3 (8b)\",\n",
    "    \"ft\" : \"Fine-tune\",\n",
    "    \"memit\" : \"MEMIT\",\n",
    "    \"lora\" : \"LoRA\",\n",
    "    \"wanda\" : \"Wanda\",\n",
    "    \"sparsegpt\" : \"SparseGPT\",\n",
    "    \"gptq\" : \"GPTQ\",\n",
    "    \"awq\" : \"AWQ\",\n",
    "    \"rmu\" : \"RMU\",\n",
    "    \"ga\" : \"GA\",\n",
    "    \"gd\" : \"GD\",\n",
    "}\n",
    "metrics = all_runs_df_deduplicated\n",
    "metrics[\"model_name\"] = metrics[\"model_name\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"edit\"] = metrics[\"edit\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"compression\"] = metrics[\"compression\"].apply(lambda x : rename_dict.get(x, None))\n",
    "metrics[\"unlearn\"] = metrics[\"unlearn\"].apply(lambda x : rename_dict.get(x, None))\n",
    "all_runs_df_deduplicated = metrics\n",
    "display(all_runs_df_deduplicated.value_counts(\"tag\"))\n",
    "print(f\"Number of experiments: {len(all_runs_df_deduplicated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a second pair of eyes on this this math\n",
    "\n",
    "# Math for determining number of interventions\n",
    "awq_settings = 6\n",
    "gptq_settings = 4 # only support quantize to [2, 3, 4, 8] bits.\n",
    "wanda_count = 6\n",
    "sparsegpt_count = 6\n",
    "editor_settings = 3\n",
    "composition_factor = 2\n",
    "\n",
    "editor_count = composition_factor * (awq_settings + gptq_settings + wanda_count + sparsegpt_count + 1) * editor_settings\n",
    "print(editor_count // 2)\n",
    "\n",
    "rmu_count = composition_factor * (awq_settings + gptq_settings + wanda_count + sparsegpt_count + editor_settings)\n",
    "print(rmu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_runs_df_deduplicated\n",
    "\n",
    "categories = {\n",
    "    \"No Intervention\": data[data[\"interventions\"].apply(lambda x: x == [])].copy(),\n",
    "    \"Editing\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\"])].copy(),\n",
    "    \"Compression\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\"])].copy(),\n",
    "    \"Edit to Compression\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\", \"compress\"])].copy(),\n",
    "    \"Compression to Edit\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\", \"edit\"])].copy(),\n",
    "    \"Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\"])].copy(),\n",
    "    \"Edit to Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"edit\", \"unlearn\"])].copy(),\n",
    "    \"Unlearn to Edit\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"edit\"])].copy(),\n",
    "    \"Compress to Unlearn\": data[data[\"interventions\"].apply(lambda x: x == [\"compress\", \"unlearn\"])].copy(),\n",
    "    \"Unlearn to Compress\": data[data[\"interventions\"].apply(lambda x: x == [\"unlearn\", \"compress\"])].copy()\n",
    "}\n",
    "\n",
    "assert len(categories[\"No Intervention\"]) == 1, f\"{len(categories['No Intervention'])} != 1\"\n",
    "assert len(categories[\"Editing\"]) == 3, f\"{len(categories['Editing'])} != 3\"\n",
    "\n",
    "# display(categories[\"Compression\"])\n",
    "assert len(categories[\"Compression\"]) == (awq_settings + gptq_settings + wanda_count + sparsegpt_count), f\"{len(categories['Compression'])} != {awq_settings + gptq_settings + wanda_count + sparsegpt_count}\"\n",
    "\n",
    "# assert len(categories[\"Edit to Compression\"]) == editor_count // 2, f\"{len(categories['Edit to Compression'])} != {editor_count // 2}\"\n",
    "\n",
    "assert len(categories[\"Compression to Edit\"]) == (editor_count // 2 )- 3, f\"{len(categories['Compression to Edit'])} != {editor_count // 2}\" # TODO: Fix this by getting the latest results\n",
    "assert len(categories[\"Unlearn\"]) == 1, f\"{len(categories['Unlearn'])} != 1\"\n",
    "assert len(categories[\"Edit to Unlearn\"]) == 9\n",
    "assert len(categories[\"Unlearn to Edit\"]) == 9, f\"{len(categories['Unlearn to Edit'])} != 3\"\n",
    "\n",
    "# display(categories[\"Compress to Unlearn\"])\n",
    "# assert len(categories[\"Compress to Unlearn\"]) == rmu_count // 2, f\"{len(categories['Compress to Unlearn'])} != {rmu_count // 2}\"\n",
    "\n",
    "# display(categories[\"Unlearn to Compress\"])\n",
    "# assert len(categories[\"Unlearn to Compress\"]) == rmu_count // 2, f\"{len(categories['Unlearn to Compress'])} != {rmu_count // 2}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Results Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_flops(value):\n",
    "    \"\"\" Format FLOPs with three significant figures and appropriate suffix. \"\"\"\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            value = clean_numeric_value(value)\n",
    "        if abs(value) < 1e6:  # Less than 1 million (below Mega)\n",
    "            return \"{:.3g}k\".format(value / 1e3)\n",
    "        elif abs(value) < 1e9:  # Mega to Giga range\n",
    "            return \"{:.3g}M\".format(value / 1e6)\n",
    "        elif abs(value) < 1e12:  # Giga to Tera range\n",
    "            return \"{:.3g}G\".format(value / 1e9)\n",
    "        else:  # Tera and above\n",
    "            return \"{:.3g}T\".format(value / 1e12)\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting FLOPs value {value}: {e}\")\n",
    "        return \"---\"\n",
    "\n",
    "def escape_latex_special_chars(s):\n",
    "    \"\"\" Escape special characters in LaTeX strings. \"\"\"\n",
    "    return str(s).replace('%', '\\\\%').replace('_', '\\\\_').replace('&', '\\\\&').replace('#', '\\\\#').replace('$', '\\\\$')\n",
    "\n",
    "def clean_numeric_value(value):\n",
    "    \"\"\" Convert a string with units to a numeric value. \"\"\"\n",
    "    try:\n",
    "        value = str(value)\n",
    "        if ' TFLOPS' in value:\n",
    "            return float(value.replace(' TFLOPS', '')) * 1e12\n",
    "        if ' GFLOPS' in value:\n",
    "            return float(value.replace(' GFLOPS', '')) * 1e9\n",
    "        if ' MFLOPS' in value:\n",
    "            return float(value.replace(' MFLOPS', '')) * 1e6\n",
    "        if ' kFLOPS' in value:\n",
    "            return float(value.replace(' kFLOPS', '')) * 1e3\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning value {value}: {e}\")\n",
    "        return pd.NA\n",
    "\n",
    "def categorize_and_generate_latex(data):\n",
    "    # Define categories based on the provided criteria\n",
    "    categories = {\n",
    "    \"No Intervention\": data[data['interventions'].apply(lambda x: x == [])].copy(),\n",
    "    \"Editing\": data[data['interventions'].apply(lambda x: x == ['edit'])].copy(),\n",
    "    \"Compression\": data[data['interventions'].apply(lambda x: x == ['compress'])].copy(),\n",
    "    \"Edit to Compression\": data[data['interventions'].apply(lambda x: x == ['edit', 'compress'])].copy(),\n",
    "    \"Compression to Edit\": data[data['interventions'].apply(lambda x: x == ['compress', 'edit'])].copy(),\n",
    "    \"Unlearn\": data[data['interventions'].apply(lambda x: x == ['unlearn'])].copy(),\n",
    "    \"Edit to Unlearn\": data[data['interventions'].apply(lambda x: x == ['edit', 'unlearn'])].copy(),\n",
    "    \"Unlearn to Edit\": data[data['interventions'].apply(lambda x: x == ['unlearn', 'edit'])].copy(),\n",
    "    \"Compress to Unlearn\": data[data['interventions'].apply(lambda x: x == ['compress', 'unlearn'])].copy(),\n",
    "    \"Unlearn to Compress\": data[data['interventions'].apply(lambda x: x == ['unlearn', 'compress'])].copy()\n",
    "}\n",
    "    # Clean numeric columns\n",
    "    for col in [\"FLOPs\", \"Latency\"]:\n",
    "        if col in data.columns:\n",
    "            data.loc[:, col] = data[col].apply(clean_numeric_value)\n",
    "            data.loc[:, col] = pd.to_numeric(data[col], errors='coerce')  # Ensure all values are numeric\n",
    "\n",
    "    # Column mappings\n",
    "    column_mappings = {\n",
    "        \"Success\": \"Rewrite accuracy\",\n",
    "        \"Generalization\": \"Generalization\",\n",
    "        \"Locality\": \"Locality\",\n",
    "        \"Avg. Bits\": \"Average bits\",\n",
    "        \"FLOPs\": \"FLOPs\",\n",
    "        \"PPL\": \"PPL\",\n",
    "        \"MMLU\": \"mmlu accuracy\",\n",
    "        \"WMDP Bio\": \"wmdp_bio accuracy\",\n",
    "        \"WMDP Cyber\": \"wmdp_cyber accuracy\"\n",
    "    }\n",
    "    latex_columns = [\"Success\", \"Generalization\", \"Locality\", \"Avg. Bits\", \"FLOPs\", \"PPL\", \"MMLU\", \"WMDP Bio\", \"WMDP Cyber\"]\n",
    "\n",
    "    # Initialize output string\n",
    "    output_str = \"\"\n",
    "\n",
    "    for category, group in categories.items():\n",
    "        if group.empty:\n",
    "            continue\n",
    "        # output_str += f\"\\\\textbf{{{category}}} \\\\\\\\ \\\\midline\\n\"\n",
    "        for _, row in group.iterrows():\n",
    "            # Calculate mean and std for each relevant column within the group\n",
    "            stats = {}\n",
    "            for latex_col, csv_col in column_mappings.items():\n",
    "                if csv_col in row.index:\n",
    "                    value = row[csv_col]\n",
    "                    if pd.isna(value):\n",
    "                        stats[latex_col] = \"---\"\n",
    "                    else:\n",
    "                        # Custom formatting for FLOPs and Latency\n",
    "                        if latex_col == \"FLOPs\":\n",
    "                            mean_str = format_flops(value)\n",
    "                            stats[latex_col] = escape_latex_special_chars(mean_str)\n",
    "                        elif latex_col == \"Latency\":\n",
    "                            mean_str = f\"{value:.3f}s\"\n",
    "                            stats[latex_col] = escape_latex_special_chars(mean_str)\n",
    "                        else:\n",
    "                            mean_str = f\"{value:.3f}\"\n",
    "                            stats[latex_col] = escape_latex_special_chars(mean_str)\n",
    "                else:\n",
    "                    stats[latex_col] = \"---\"\n",
    "\n",
    "            # Prepare the LaTeX row for the current group\n",
    "            latex_row = escape_latex_special_chars(row['tag'])  # Use the tag name directly without escaping\n",
    "            for column in latex_columns:\n",
    "                latex_row += \" & \" + stats.get(column, \"---\")\n",
    "            latex_row += \" \\\\\\\\\"\n",
    "\n",
    "            # Append to output string\n",
    "            output_str += latex_row + \"\\n\"\n",
    "        \n",
    "        output_str += \"\\\\midrule\\n\"\n",
    "\n",
    "    return output_str\n",
    "\n",
    "latex_rows_with_categories = categorize_and_generate_latex(all_runs_df_deduplicated)\n",
    "print(latex_rows_with_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the font family to serif\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.color_palette(\"pastel\")\n",
    "\n",
    "# plotting constants\n",
    "TITLE_FONT_SIZE = 18\n",
    "LEGEND_FONT_SIZE = 12\n",
    "WSPACE = 0.3\n",
    "FIGURE_HEIGHT = 3\n",
    "LINE_WIDTH = 2\n",
    "FIG_SIZE = 3\n",
    "MARKER_SIZE = 8\n",
    "X_LABEL_ROTATION = 20\n",
    "\n",
    "# Set colors for compositons with compression\n",
    "colors = {\"Wanda\": \"C1\", \"SparseGPT\": \"C2\", \"AWQ\": \"C3\", \"GPTQ\": \"C4\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Composability Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intervention names and types\n",
    "intervention_names = [intervention for intervention in list(data[\"edit\"].unique()) + list(data[\"unlearn\"].unique()) + list(data[\"compression\"].unique()) if intervention is not None]\n",
    "intervention_type = {\n",
    "    \"LoRA\": \"edit\",\n",
    "    \"MEMIT\": \"edit\",\n",
    "    \"Fine-tune\": \"edit\",\n",
    "    \"SparseGPT\": \"compression\",\n",
    "    \"Wanda\": \"compression\",\n",
    "    \"GPTQ\": \"compression\",\n",
    "    \"AWQ\": \"compression\",\n",
    "    \"RMU\": \"unlearn\",\n",
    "    \"GA\": \"unlearn\",\n",
    "    \"GD\": \"unlearn\",\n",
    "}\n",
    "\n",
    "# Initialize heatmap data frames with default values\n",
    "default_value = None\n",
    "mmlu_heatmap_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "wmdp_heatmap_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "rewrite_heatmap_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "generalization_heatmap_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "\n",
    "# Initialize max value data frames\n",
    "mmlu_max_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "wmdp_max_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "rewrite_max_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "generalization_max_data = pd.DataFrame(index=intervention_names, columns=intervention_names, dtype=float, data=default_value)\n",
    "\n",
    "# Populate the heatmap and max value data frames\n",
    "for first_intervention in intervention_names:\n",
    "    for second_intervention in intervention_names:\n",
    "        first_intervention_type = intervention_type[first_intervention]\n",
    "        second_intervention_type = intervention_type[second_intervention]\n",
    "        if first_intervention_type == second_intervention_type:\n",
    "            continue\n",
    "\n",
    "        compositions = data[(data[first_intervention_type] == first_intervention) & (data[second_intervention_type] == second_intervention)]\n",
    "        if first_intervention in [\"SparseGPT\", \"Wanda\"] or second_intervention in [\"SparseGPT\", \"Wanda\"]:\n",
    "            compositions = compositions[compositions[\"sparsity_ratio\"] == 0.25]\n",
    "        elif first_intervention in [\"GPTQ\", \"AWQ\"] or second_intervention in [\"GPTQ\", \"AWQ\"]:\n",
    "            compositions = compositions[compositions[\"wbits\"] == 4]\n",
    "        \n",
    "        assert len(compositions) == 2, f\"Expected 2 compositions for {first_intervention} and {second_intervention}, but found {len(compositions)}\"\n",
    "        \n",
    "        # Calculate differences\n",
    "        mmlu_diff = abs(compositions[\"mmlu accuracy\"].iloc[0] - compositions[\"mmlu accuracy\"].iloc[1]).round(4)\n",
    "        mmlu_heatmap_data[first_intervention][second_intervention] = mmlu_diff\n",
    "        \n",
    "        avg_wmdp_diff = abs(((compositions.iloc[0][\"wmdp_cyber accuracy\"] + compositions.iloc[0][\"wmdp_bio accuracy\"]) / 2) - ((compositions.iloc[1][\"wmdp_cyber accuracy\"] + compositions.iloc[1][\"wmdp_bio accuracy\"]) / 2)).round(4)\n",
    "        wmdp_heatmap_data[first_intervention][second_intervention] = avg_wmdp_diff\n",
    "        \n",
    "        rewrite_diff = abs(compositions[\"Rewrite accuracy\"].iloc[0] - compositions[\"Rewrite accuracy\"].iloc[1]).round(4)\n",
    "        rewrite_heatmap_data[first_intervention][second_intervention] = rewrite_diff\n",
    "\n",
    "        generalization_diff = abs(compositions[\"Generalization\"].iloc[0] - compositions[\"Generalization\"].iloc[1]).round(4)\n",
    "        generalization_heatmap_data[first_intervention][second_intervention] = generalization_diff\n",
    "        \n",
    "        # Calculate max values\n",
    "        mmlu_max = max(compositions[\"mmlu accuracy\"].iloc[0], compositions[\"mmlu accuracy\"].iloc[1]).round(4)\n",
    "        mmlu_max_data[first_intervention][second_intervention] = mmlu_max\n",
    "        \n",
    "        avg_wmdp_max = min((compositions.iloc[0][\"wmdp_cyber accuracy\"] + compositions.iloc[0][\"wmdp_bio accuracy\"]) / 2, (compositions.iloc[1][\"wmdp_cyber accuracy\"] + compositions.iloc[1][\"wmdp_bio accuracy\"]) / 2).round(4)\n",
    "        wmdp_max_data[first_intervention][second_intervention] = avg_wmdp_max\n",
    "        \n",
    "        rewrite_max = max(compositions[\"Rewrite accuracy\"].iloc[0], compositions[\"Rewrite accuracy\"].iloc[1]).round(4)\n",
    "        rewrite_max_data[first_intervention][second_intervention] = rewrite_max\n",
    "\n",
    "        generalization_max = max(compositions[\"Generalization\"].iloc[0], compositions[\"Generalization\"].iloc[1]).round(4)\n",
    "        generalization_max_data[first_intervention][second_intervention] = generalization_max\n",
    "\n",
    "# Display the results\n",
    "print(\"MMLU Difference\")\n",
    "display(mmlu_heatmap_data)\n",
    "\n",
    "print(\"MMLU Max Values\")\n",
    "display(mmlu_max_data)\n",
    "\n",
    "print(\"WMDP Difference\")\n",
    "display(wmdp_heatmap_data)\n",
    "\n",
    "print(\"WMDP Max Values\")\n",
    "display(wmdp_max_data)\n",
    "\n",
    "print(\"Rewrite Difference\")\n",
    "display(rewrite_heatmap_data)\n",
    "\n",
    "print(\"Rewrite Max Values\")\n",
    "display(rewrite_max_data)\n",
    "\n",
    "print(\"Generalization Difference\")\n",
    "display(generalization_heatmap_data)\n",
    "\n",
    "print(\"Generalization Max Values\")\n",
    "display(generalization_max_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_value(value):\n",
    "    if pd.isnull(value):\n",
    "        return ''\n",
    "    elif value == 1:\n",
    "        return '1.000'\n",
    "    else:\n",
    "        return f'{value:.3f}'[1:] if value < 1 else f'{value:.3f}'\n",
    "\n",
    "def generate_latex_table(edit_max_df, edit_diff_df, gen_max_df, gen_diff_df, mmlu_max_df, mmlu_diff_df, edit_interventions, mmlu_interventions, title, label):\n",
    "    latex_code = r'''\\begin{table}[t]\n",
    "    \\centering\n",
    "    \\resizebox{\\linewidth}{!}{\n",
    "    \\begin{tabular}{lcccccccccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} & \\multicolumn{6}{c}{\\textbf{MMLU}}\\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13} \\cmidrule(lr){14-19}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}}\\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19}\n",
    "        \\textbf{Method} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT}\\\\\n",
    "        \\midrule\n",
    "'''\n",
    "    \n",
    "    for method in ['Wanda', 'SparseGPT', 'AWQ', 'GPTQ']:\n",
    "        latex_code += f'        \\\\textbf{{{method}}} '\n",
    "        \n",
    "        # Edit Success - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(edit_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Edit Success - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(edit_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "\n",
    "        # Generalization - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(gen_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Generalization - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(gen_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        # MMLU - MCS\n",
    "        for intervention in mmlu_interventions:\n",
    "            mcs_value = format_value(mmlu_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # MMLU - OI\n",
    "        for intervention in mmlu_interventions:\n",
    "            oi_value = format_value(mmlu_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        latex_code += '\\\\\\\\\\n'\n",
    "        if method == 'SparseGPT':\n",
    "            latex_code += '        \\\\cdashlinelr{1-19}\\n'\n",
    "    \n",
    "    # Calculate and add averages for each method\n",
    "    latex_code += '        \\\\cdashlinelr{1-19}\\n'\n",
    "    latex_code += '        \\\\textit{Average} '\n",
    "\n",
    "    # Calculate averages for each section and add to LaTeX code\n",
    "    def calculate_averages(df, interventions):\n",
    "        averages = []\n",
    "        for intervention in interventions:\n",
    "            avg_value = format_value(df[intervention].mean())\n",
    "            averages.append(avg_value)\n",
    "        return averages\n",
    "\n",
    "    # Averages for Edit Success - MCS and OI\n",
    "    avg_edit_mcs = calculate_averages(edit_max_df, edit_interventions)\n",
    "    avg_edit_oi = calculate_averages(edit_diff_df, edit_interventions)\n",
    "    avg_gen_mcs = calculate_averages(gen_max_df, edit_interventions)\n",
    "    avg_gen_oi = calculate_averages(gen_diff_df, edit_interventions)\n",
    "    avg_mmlu_mcs = calculate_averages(mmlu_max_df, mmlu_interventions)\n",
    "    avg_mmlu_oi = calculate_averages(mmlu_diff_df, mmlu_interventions)\n",
    "\n",
    "    # Append averages to the LaTeX code\n",
    "    for avg_value in avg_edit_mcs + avg_edit_oi + avg_gen_mcs + avg_gen_oi + avg_mmlu_mcs + avg_mmlu_oi:\n",
    "        latex_code += f'& {avg_value} '\n",
    "\n",
    "    latex_code += '\\\\\\\\\\n'\n",
    "    \n",
    "    latex_code += r'''        \\bottomrule\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    \\caption{''' + title + r'''}\n",
    "    \\label{''' + label + r'''}\n",
    "\\end{table}\n",
    "'''\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Assuming rewrite_max_data, rewrite_heatmap_data, mmlu_max_data, and mmlu_heatmap_data are defined dataframes\n",
    "# Assuming generalization_max_data and generalization_heatmap_data are defined dataframes\n",
    "edit_interventions = ['Fine-tune', 'LoRA', 'MEMIT']\n",
    "mmlu_interventions = ['Fine-tune', 'LoRA', 'MEMIT']\n",
    "title = \"Editing and Compression\"\n",
    "label = \"tab:edit_compress\"\n",
    "\n",
    "latex_table_code = generate_latex_table(rewrite_max_data, rewrite_heatmap_data, generalization_max_data, generalization_heatmap_data, mmlu_max_data, mmlu_heatmap_data, edit_interventions, mmlu_interventions, title, label)\n",
    "print(latex_table_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_value(value):\n",
    "    if pd.isnull(value):\n",
    "        return ''\n",
    "    elif value == 1:\n",
    "        return '1.000'\n",
    "    else:\n",
    "        return f'{value:.3f}'[1:] if value < 1 else f'{value:.3f}'\n",
    "\n",
    "def generate_latex_table(edit_max_df, edit_diff_df, gen_max_df, gen_diff_df, mmlu_max_df, mmlu_diff_df, edit_interventions, mmlu_interventions, title, label):\n",
    "    latex_code = r'''\\begin{table}[t]\n",
    "    \\centering\n",
    "    \\resizebox{\\linewidth}{!}{\n",
    "    \\begin{tabular}{lcccccccccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{6}{c}{\\textbf{Edit Success}} & \\multicolumn{6}{c}{\\textbf{Generalization}} & \\multicolumn{6}{c}{\\textbf{MMLU}}\\\\\n",
    "        \\cmidrule(lr){2-7} \\cmidrule(lr){8-13} \\cmidrule(lr){14-19}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}}\\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19}\n",
    "        \\textbf{Method} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT}\\\\\n",
    "        \\midrule\n",
    "'''\n",
    "    \n",
    "    for method in ['Wanda', 'SparseGPT', 'AWQ', 'GPTQ']:\n",
    "        latex_code += f'        \\\\textbf{{{method}}} '\n",
    "        \n",
    "        # Edit Success - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(edit_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Edit Success - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(edit_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "\n",
    "        # Generalization - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(gen_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Generalization - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(gen_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        # MMLU - MCS\n",
    "        for intervention in mmlu_interventions:\n",
    "            mcs_value = format_value(mmlu_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # MMLU - OI\n",
    "        for intervention in mmlu_interventions:\n",
    "            oi_value = format_value(mmlu_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        latex_code += '\\\\\\\\\\n'\n",
    "        if method == 'SparseGPT':\n",
    "            latex_code += '        \\\\cdashlinelr{1-19}\\n'\n",
    "    \n",
    "    latex_code += r'''        \\bottomrule\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    \\caption{''' + title + r'''}\n",
    "    \\label{''' + label + r'''}\n",
    "\\end{table}\n",
    "'''\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "# Assuming rewrite_max_data, rewrite_heatmap_data, mmlu_max_data, and mmlu_heatmap_data are defined dataframes\n",
    "# Assuming generalization_max_data and generalization_heatmap_data are defined dataframes\n",
    "edit_interventions = ['Fine-tune', 'LoRA', 'MEMIT']\n",
    "mmlu_interventions = ['Fine-tune', 'LoRA', 'MEMIT']\n",
    "title = \"Editing and Compression\"\n",
    "label = \"tab:edit_compress\"\n",
    "\n",
    "latex_table_code = generate_latex_table(rewrite_max_data, rewrite_heatmap_data, generalization_max_data, generalization_heatmap_data, mmlu_max_data, mmlu_heatmap_data, edit_interventions, mmlu_interventions, title, label)\n",
    "print(latex_table_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_value(value):\n",
    "    if pd.isnull(value):\n",
    "        return ''\n",
    "    elif value == 1:\n",
    "        return '1.000'\n",
    "    else:\n",
    "        return f'{value:.3f}'[1:] if value < 1 else f'{value:.3f}'\n",
    "\n",
    "def generate_unlearning_table(edit_max_df, edit_diff_df, gen_max_df, gen_diff_df, mmlu_max_df, mmlu_diff_df, wmdp_max_df, wmdp_diff_df, edit_interventions, mmlu_interventions, title, label):\n",
    "    latex_code = r'''\\begin{table}[t]\n",
    "    \\centering\n",
    "    \\resizebox{\\linewidth}{!}{\n",
    "    \\begin{tabular}{lcccccccccccccccccccccccccc}\n",
    "        \\toprule\n",
    "        & \\multicolumn{12}{c}{\\textbf{Edit Success}} & \\multicolumn{12}{c}{\\textbf{Generalization}}\\\\\n",
    "        \\cmidrule(lr){2-13} \\cmidrule(lr){14-25}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}}\\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19} \\cmidrule(lr){20-22} \\cmidrule(lr){23-25}\n",
    "        \\textbf{Method} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT}\\\\\n",
    "        \\midrule\n",
    "'''\n",
    "    \n",
    "    for method in ['GA', 'GD', 'RMU']:\n",
    "        latex_code += f'        \\\\textbf{{{method}}} '\n",
    "        \n",
    "        # Edit Success - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(edit_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Edit Success - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(edit_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "\n",
    "        # Generalization - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(gen_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # Generalization - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(gen_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        latex_code += '\\\\\\\\\\n'\n",
    "        if method == 'GD':\n",
    "            latex_code += '        \\\\cdashlinelr{1-25}\\n'\n",
    "    \n",
    "    # Calculate and add average for Edit Success and Generalization\n",
    "    latex_code += '        \\\\cdashlinelr{1-25}\\n'\n",
    "    latex_code += '        \\\\textit{Average} '\n",
    "    \n",
    "    for dfs in [edit_max_df, edit_diff_df, gen_max_df, gen_diff_df]:\n",
    "        for intervention in edit_interventions:\n",
    "            avg_value = format_value(dfs[intervention].mean())\n",
    "            latex_code += f'& {avg_value} '\n",
    "    \n",
    "    latex_code += '\\\\\\\\\\n'\n",
    "    \n",
    "    latex_code += r'''        \\midrule\n",
    "        & \\multicolumn{12}{c}{\\textbf{WMDP}} & \\multicolumn{12}{c}{\\textbf{MMLU}}\\\\\n",
    "        \\cmidrule(lr){2-13} \\cmidrule(lr){14-25}\n",
    "        & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}} & \\multicolumn{3}{c}{\\textbf{MCS}} & \\multicolumn{3}{c}{\\textbf{OI}}\\\\\n",
    "        \\cmidrule(lr){2-4} \\cmidrule(lr){5-7} \\cmidrule(lr){8-10} \\cmidrule(lr){11-13} \\cmidrule(lr){14-16} \\cmidrule(lr){17-19} \\cmidrule(lr){20-22} \\cmidrule(lr){23-25}\n",
    "        \\textbf{Method} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT} & \\textbf{Finetune} & \\textbf{LoRA} & \\textbf{MEMIT}\\\\\n",
    "        \\midrule\n",
    "'''\n",
    "    \n",
    "    for method in ['GA', 'GD', 'RMU']:\n",
    "        latex_code += f'        \\\\textbf{{{method}}} '\n",
    "        \n",
    "        # WMDP - MCS\n",
    "        for intervention in edit_interventions:\n",
    "            mcs_value = format_value(wmdp_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # WMDP - OI\n",
    "        for intervention in edit_interventions:\n",
    "            oi_value = format_value(wmdp_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        # MMLU - MCS\n",
    "        for intervention in mmlu_interventions:\n",
    "            mcs_value = format_value(mmlu_max_df.at[method, intervention])\n",
    "            latex_code += f'& {mcs_value} '\n",
    "        \n",
    "        # MMLU - OI\n",
    "        for intervention in mmlu_interventions:\n",
    "            oi_value = format_value(mmlu_diff_df.at[method, intervention])\n",
    "            latex_code += f'& {oi_value} '\n",
    "        \n",
    "        latex_code += '\\\\\\\\\\n'\n",
    "        if method == 'GD':\n",
    "            latex_code += '        \\\\cdashlinelr{1-25}\\n'\n",
    "    \n",
    "    # Calculate and add average for WMDP and MMLU\n",
    "    latex_code += '        \\\\cdashlinelr{1-25}\\n'\n",
    "    latex_code += '        \\\\textit{Average} '\n",
    "    \n",
    "    for dfs in [wmdp_max_df, wmdp_diff_df, mmlu_max_df, mmlu_diff_df]:\n",
    "        for intervention in mmlu_interventions:\n",
    "            avg_value = format_value(dfs[intervention].mean())\n",
    "            latex_code += f'& {avg_value} '\n",
    "    \n",
    "    latex_code += '\\\\\\\\\\n'\n",
    "    \n",
    "    latex_code += r'''        \\bottomrule\n",
    "    \\end{tabular}\n",
    "    }\n",
    "    \\caption{''' + title + r'''}\n",
    "    \\label{''' + label + r'''}\n",
    "\\end{table}\n",
    "'''\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "latex_table_code = generate_unlearning_table(rewrite_max_data, rewrite_heatmap_data, generalization_max_data, generalization_heatmap_data, mmlu_max_data, mmlu_heatmap_data, wmdp_max_data, wmdp_heatmap_data, edit_interventions, mmlu_interventions, title, label)\n",
    "print(latex_table_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three heatmaps next two eachother, one for each frame\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6 * FIG_SIZE, 2 * FIG_SIZE))\n",
    "y_labels = {\n",
    "    0: \"MMLU Acc\",\n",
    "    1: \"WMDP Acc\",\n",
    "    2: \"Edit Success\"\n",
    "}\n",
    "\n",
    "sns.heatmap(mmlu_heatmap_data, annot=True, fmt=\".1%\", cmap=\"coolwarm\", ax=axes[0], cbar=False)\n",
    "sns.heatmap(wmdp_heatmap_data, annot=True, fmt=\".1%\", cmap=\"coolwarm\", ax=axes[1], cbar=False)\n",
    "sns.heatmap(rewrite_heatmap_data, annot=True, fmt=\".1%\", cmap=\"coolwarm\", ax=axes[2], cbar=False)\n",
    "\n",
    "# roate x labels\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=X_LABEL_ROTATION)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_ylabel(y_labels[i], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "\n",
    "if not os.path.exists(\"figures\"):\n",
    "    os.makedirs(\"figures\")\n",
    "\n",
    "plt.savefig(\"figures/delta_heatmaps.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Editing under Compression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_label(row):\n",
    "    interventions = row[\"interventions\"]\n",
    "    first_method = \"\"\n",
    "    second_method = \"\"\n",
    "    if interventions[0] == \"edit\":\n",
    "        first_method = row[\"edit\"]\n",
    "    elif interventions[0] == \"compress\":\n",
    "        first_method = row[\"compression\"]\n",
    "    elif interventions[0] == \"unlearn\":\n",
    "        first_method = row[\"unlearn\"]\n",
    "    \n",
    "    if interventions[1] == \"edit\":\n",
    "        second_method = row[\"edit\"]\n",
    "    elif interventions[1] == \"compress\":\n",
    "        second_method = row[\"compression\"]\n",
    "    elif interventions[1] == \"unlearn\":\n",
    "        second_method = row[\"unlearn\"]\n",
    "    \n",
    "    return f\"{first_method}â†’{second_method}\"\n",
    "\n",
    "def wrap_label(interventions):\n",
    "    first_intervention, second_intervention = interventions[0], interventions[1]\n",
    "    first_letter_upper = first_intervention[0].upper()\n",
    "    second_letter_upper = second_intervention[0].upper()\n",
    "    \n",
    "    # EX: E $\\rightarrow$ C\n",
    "    return f\"{first_letter_upper}$\\\\rightarrow${second_letter_upper}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mock records for baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want instances where editing has been applied but there is no unlearning or compression. In these cases, set wbits=16 and sparsity=0 \n",
    "baseline_editors = data[(data[\"edit\"].notnull()) & (data[\"unlearn\"].isnull()) & (data[\"compression\"].isnull()) & (data[\"interventions\"].apply(lambda x: x == [\"edit\"]))].copy()\n",
    "baseline_editors[\"wbits\"] = 16\n",
    "baseline_editors[\"sparsity_ratio\"] = 0\n",
    "news_records = []\n",
    "\n",
    "# Edit and Compress\n",
    "for editing_method in [\"LoRA\", \"MEMIT\", \"Fine-tune\"]:\n",
    "    baseline_record = baseline_editors[baseline_editors[\"edit\"] == editing_method]\n",
    "    for compression_method in [\"SparseGPT\", \"Wanda\", \"GPTQ\", \"AWQ\"]:\n",
    "        edit_first_record = baseline_record.copy()\n",
    "        edit_first_record[\"compression\"] = compression_method\n",
    "        edit_first_record[\"interventions\"] = [[\"edit\", \"compress\"]]\n",
    "        news_records.append(edit_first_record)\n",
    "\n",
    "        compress_first_record = baseline_record.copy()\n",
    "        compress_first_record[\"compression\"] = compression_method\n",
    "        compress_first_record[\"interventions\"] = [[\"compress\", \"edit\"]]\n",
    "        news_records.append(compress_first_record)\n",
    "\n",
    "baseline_unlearners = data[(data[\"edit\"].isnull()) & (data[\"unlearn\"].notnull()) & (data[\"compression\"].isnull()) & (data[\"interventions\"].apply(lambda x: x == [\"unlearn\"]))].copy()\n",
    "\n",
    "# Compress and Unlearn\n",
    "for unlearn_method in [\"RMU\"]:\n",
    "    baseline_record = baseline_unlearners[baseline_unlearners[\"unlearn\"] == unlearn_method]\n",
    "\n",
    "    for compression_method in [\"SparseGPT\", \"Wanda\", \"GPTQ\", \"AWQ\"]:\n",
    "        compress_first_record = baseline_record.copy()\n",
    "        compress_first_record[\"unlearn\"] = unlearn_method\n",
    "        compress_first_record[\"compression\"] = compression_method\n",
    "        compress_first_record[\"interventions\"] = [[\"compress\", \"unlearn\"]]\n",
    "        news_records.append(compress_first_record)\n",
    "\n",
    "        unlearn_first_record = baseline_record.copy()\n",
    "        unlearn_first_record[\"unlearn\"] = unlearn_method\n",
    "        unlearn_first_record[\"compression\"] = compression_method\n",
    "        unlearn_first_record[\"interventions\"] = [[\"unlearn\", \"compress\"]]\n",
    "        news_records.append(unlearn_first_record)\n",
    "\n",
    "baseline_records = pd.concat(news_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Pruning and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mock baseline records to the frame used for plotting\n",
    "data = pd.concat([data, baseline_records])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame = data[((data[\"compression\"] == \"SparseGPT\") | (data[\"compression\"] == \"Wanda\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "# pruning_frame = pruning_frame[pruning_frame[\"edit\"] != \"Fine-tune\"]\n",
    "pruning_frame[\"order\"] = pruning_frame.apply(get_order_label, axis=1)\n",
    "pruning_frame = pruning_frame.sort_values(by=\"order\")\n",
    "\n",
    "quantization_frame = data[((data[\"compression\"] == \"GPTQ\") | (data[\"compression\"] == \"AWQ\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "quantization_frame = quantization_frame[quantization_frame[\"edit\"] != \"Fine-tune\"]\n",
    "quantization_frame[\"order\"] = quantization_frame.apply(get_order_label, axis=1)\n",
    "quantization_frame = quantization_frame.sort_values(by=\"order\")\n",
    "\n",
    "# 4 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 4, figsize=(5 * FIG_SIZE, 3 * FIG_SIZE))\n",
    "row_metrics = {\n",
    "    0: \"Rewrite accuracy\",\n",
    "    1: \"Generalization\",\n",
    "    2: \"mmlu accuracy\",\n",
    "}\n",
    "row_labels = {\n",
    "    0: r\"Edit Success$ \\uparrow$\",\n",
    "    1: r\"Generalization$ \\uparrow$\",\n",
    "    2: r\"MMLU$ \\uparrow$\"\n",
    "}\n",
    "column_edit_methods = {\n",
    "    0: \"MEMIT\",\n",
    "    1: \"LoRA\",\n",
    "    2: \"MEMIT\",\n",
    "    3: \"LoRA\",\n",
    "}\n",
    "\n",
    "compositions_by_col = {\n",
    "    # MEMIT and WANDA + SparseGPT\n",
    "    0: [(\"MEMITâ†’SparseGPT\", \"SparseGPTâ†’MEMIT\"), (\"MEMITâ†’Wanda\", \"Wandaâ†’MEMIT\")],\n",
    "    # LoRA and WANDA + SparseGPT\n",
    "    1: [(\"LoRAâ†’SparseGPT\", \"SparseGPTâ†’LoRA\"), (\"LoRAâ†’Wanda\", \"Wandaâ†’LoRA\")],\n",
    "    # MEMIT and GPTQ + AWQ\n",
    "    2: [(\"MEMITâ†’GPTQ\", \"GPTQâ†’MEMIT\"), (\"MEMITâ†’AWQ\", \"AWQâ†’MEMIT\")],\n",
    "    # LoRA and GPTQ + AWQ\n",
    "    3: [(\"LoRAâ†’GPTQ\", \"GPTQâ†’LoRA\"), (\"LoRAâ†’AWQ\", \"AWQâ†’LoRA\")],\n",
    "}\n",
    "for row_index, y_metric in row_metrics.items():\n",
    "    for col_index, plotting_frame in enumerate([pruning_frame, pruning_frame, quantization_frame, quantization_frame]):\n",
    "        ax = axes[row_index][col_index]\n",
    "        x_metric = \"sparsity_ratio\" if col_index < 2 else \"wbits\"\n",
    "        plotting_frame = plotting_frame[plotting_frame[\"edit\"] == column_edit_methods[col_index]]\n",
    "\n",
    "        for composition in compositions_by_col[col_index]:\n",
    "            compression_method = [method for method in composition[0].split(\"â†’\") if method not in [\"MEMIT\", \"LoRA\", \"Fine-tune\"]][0]\n",
    "            first_line = plotting_frame[plotting_frame[\"order\"] == composition[0]]\n",
    "            first_line[\"label\"] = first_line[\"order\"].apply(wrap_label)\n",
    "            second_line = plotting_frame[plotting_frame[\"order\"] == composition[1]].sort_values(x_metric)\n",
    "            second_line[\"label\"] = second_line[\"order\"].apply(wrap_label)\n",
    "            if compression_method in [\"AWQ\", \"GPTQ\"]:\n",
    "                first_line = first_line.sort_values(x_metric, ascending=False)\n",
    "                second_line = second_line.sort_values(x_metric, ascending=False)\n",
    "            else:\n",
    "                first_line = first_line.sort_values(x_metric)\n",
    "                second_line = second_line.sort_values(x_metric)\n",
    "\n",
    "            ax.plot(first_line[x_metric], first_line[y_metric], marker=\"o\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[0]}\")\n",
    "            ax.plot(second_line[x_metric], second_line[y_metric], markerfacecolor='none', marker=\"o\", ls=\"--\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[1]}\")\n",
    "            ax.fill_between(\n",
    "                x=first_line[x_metric], y1=first_line[y_metric], y2=second_line[y_metric],\n",
    "                alpha=0.3,\n",
    "                color=colors[compression_method]\n",
    "            )\n",
    "\n",
    "        if row_index != 2:\n",
    "            ax.set_ylim(0, 1.05)\n",
    "        else:\n",
    "            ax.set_ylim(0.2, 0.65)\n",
    "            ax.axhline(y=0.25, color=\"gray\", linestyle=\"--\")\n",
    "            if col_index < 2:\n",
    "                ax.text(0.35, 0.28, \"Random\", color=\"darkgray\", ha=\"center\")\n",
    "            else:\n",
    "                ax.text(8, 0.28, \"Random\", color=\"darkgray\", ha=\"right\")\n",
    "\n",
    "        if row_index == 0:\n",
    "            title = column_edit_methods[col_index]\n",
    "            ax.set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "        if col_index == 0:\n",
    "            ax.set_ylabel(row_labels[row_index], fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.set_xlabel(\"Sparsity\" if col_index < 2 else \"Bits\", fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.legend(fontsize=LEGEND_FONT_SIZE, frameon=False, loc=\"upper center\", bbox_to_anchor=(0.5, -0.3), ncol=1)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "plt.savefig(\"figures/main_results_editors_compression.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 X 3 for Pruning and Quantization (Two Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositions_by_col = {\n",
    "    # MEMIT and WANDA + SparseGPT\n",
    "    0: [(\"MEMITâ†’SparseGPT\", \"SparseGPTâ†’MEMIT\"), (\"MEMITâ†’Wanda\", \"Wandaâ†’MEMIT\")],\n",
    "    # LoRA and WANDA + SparseGPT\n",
    "    1: [(\"LoRAâ†’SparseGPT\", \"SparseGPTâ†’LoRA\"), (\"LoRAâ†’Wanda\", \"Wandaâ†’LoRA\")],\n",
    "    # FT and WANDA + SparseGPT\n",
    "    2: [(\"Fine-tuneâ†’SparseGPT\", \"SparseGPTâ†’Fine-tune\"), (\"Fine-tuneâ†’Wanda\", \"Wandaâ†’Fine-tune\")],\n",
    "    # MEMIT and GPTQ + AWQ\n",
    "    3: [(\"MEMITâ†’GPTQ\", \"GPTQâ†’MEMIT\"), (\"MEMITâ†’AWQ\", \"AWQâ†’MEMIT\")],\n",
    "    # LoRA and GPTQ + AWQ\n",
    "    4: [(\"LoRAâ†’GPTQ\", \"GPTQâ†’LoRA\"), (\"LoRAâ†’AWQ\", \"AWQâ†’LoRA\")],\n",
    "    # FT and GPTQ + AWQ\n",
    "    5: [(\"Fine-tuneâ†’GPTQ\", \"GPTQâ†’Fine-tune\"), (\"Fine-tuneâ†’AWQ\", \"AWQâ†’Fine-tune\")],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame[\"edit\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame = data[((data[\"compression\"] == \"SparseGPT\") | (data[\"compression\"] == \"Wanda\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "pruning_frame[\"order\"] = pruning_frame.apply(get_order_label, axis=1)\n",
    "pruning_frame = pruning_frame.sort_values(by=\"order\")\n",
    "\n",
    "quantization_frame = data[((data[\"compression\"] == \"GPTQ\") | (data[\"compression\"] == \"AWQ\")) & (data[\"edit\"] != None) & (data[\"interventions\"].apply(lambda x: len(x) > 1))]\n",
    "quantization_frame[\"order\"] = quantization_frame.apply(get_order_label, axis=1)\n",
    "quantization_frame = quantization_frame.sort_values(by=\"order\")\n",
    "\n",
    "# 4 columns and 3 rows\n",
    "fig, axes = plt.subplots(3, 6, figsize=(6 * FIG_SIZE, 3 * FIG_SIZE))\n",
    "row_metrics = {\n",
    "    0: \"Rewrite accuracy\",\n",
    "    1: \"Generalization\",\n",
    "    2: \"mmlu accuracy\",\n",
    "}\n",
    "row_labels = {\n",
    "    0: r\"Edit Success$ \\uparrow$\",\n",
    "    1: r\"Generalization$ \\uparrow$\",\n",
    "    2: r\"MMLU$ \\uparrow$\"\n",
    "}\n",
    "column_edit_methods = {\n",
    "    0: \"MEMIT\",\n",
    "    1: \"LoRA\",\n",
    "    2: \"Fine-tune\",\n",
    "    3: \"MEMIT\",\n",
    "    4: \"LoRA\",\n",
    "    5: \"Fine-tune\"\n",
    "}\n",
    "\n",
    "compositions_by_col = {\n",
    "    # MEMIT and WANDA + SparseGPT\n",
    "    0: [(\"MEMITâ†’SparseGPT\", \"SparseGPTâ†’MEMIT\"), (\"MEMITâ†’Wanda\", \"Wandaâ†’MEMIT\")],\n",
    "    # LoRA and WANDA + SparseGPT\n",
    "    1: [(\"LoRAâ†’SparseGPT\", \"SparseGPTâ†’LoRA\"), (\"LoRAâ†’Wanda\", \"Wandaâ†’LoRA\")],\n",
    "    # FT and WANDA + SparseGPT\n",
    "    2: [(\"Fine-tuneâ†’SparseGPT\", \"SparseGPTâ†’Fine-tune\"), (\"Fine-tuneâ†’Wanda\", \"Wandaâ†’Fine-tune\")],\n",
    "    # MEMIT and GPTQ + AWQ\n",
    "    3: [(\"MEMITâ†’GPTQ\", \"GPTQâ†’MEMIT\"), (\"MEMITâ†’AWQ\", \"AWQâ†’MEMIT\")],\n",
    "    # LoRA and GPTQ + AWQ\n",
    "    4: [(\"LoRAâ†’GPTQ\", \"GPTQâ†’LoRA\"), (\"LoRAâ†’AWQ\", \"AWQâ†’LoRA\")],\n",
    "    # FT and GPTQ + AWQ\n",
    "    5: [(\"Fine-tuneâ†’GPTQ\", \"GPTQâ†’Fine-tune\"), (\"Fine-tuneâ†’AWQ\", \"AWQâ†’Fine-tune\")],\n",
    "}\n",
    "for row_index, y_metric in row_metrics.items():\n",
    "    for col_index, plotting_frame in enumerate([pruning_frame, pruning_frame, pruning_frame, quantization_frame, quantization_frame, quantization_frame]):\n",
    "        ax = axes[row_index][col_index]\n",
    "        x_metric = \"sparsity_ratio\" if col_index < 3 else \"wbits\"\n",
    "        plotting_frame = plotting_frame[plotting_frame[\"edit\"] == column_edit_methods[col_index]]\n",
    "\n",
    "        for composition in compositions_by_col[col_index]:\n",
    "            compression_method = [method for method in composition[0].split(\"â†’\") if method not in [\"MEMIT\", \"LoRA\", \"Fine-tune\"]][0]\n",
    "            first_line = plotting_frame[plotting_frame[\"order\"] == composition[0]]\n",
    "            first_line[\"label\"] = first_line[\"order\"].apply(wrap_label)\n",
    "            second_line = plotting_frame[plotting_frame[\"order\"] == composition[1]].sort_values(x_metric)\n",
    "            second_line[\"label\"] = second_line[\"order\"].apply(wrap_label)\n",
    "            if compression_method in [\"AWQ\", \"GPTQ\"]:\n",
    "                first_line = first_line.sort_values(x_metric, ascending=False)\n",
    "                second_line = second_line.sort_values(x_metric, ascending=False)\n",
    "            else:\n",
    "                first_line = first_line.sort_values(x_metric)\n",
    "                second_line = second_line.sort_values(x_metric)\n",
    "\n",
    "            ax.plot(first_line[x_metric], first_line[y_metric], marker=\"o\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[0]}\")\n",
    "            ax.plot(second_line[x_metric], second_line[y_metric], markerfacecolor='none', marker=\"o\", ls=\"--\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{composition[1]}\")\n",
    "            ax.fill_between(\n",
    "                x=first_line[x_metric], y1=first_line[y_metric], y2=second_line[y_metric],\n",
    "                alpha=0.3,\n",
    "                color=colors[compression_method]\n",
    "            )\n",
    "\n",
    "        if row_index != 2:\n",
    "            ax.set_ylim(0, 1.05)\n",
    "        else:\n",
    "            ax.set_ylim(0.2, 0.65)\n",
    "            ax.axhline(y=0.25, color=\"gray\", linestyle=\"--\")\n",
    "            if col_index < 3:\n",
    "                ax.text(0.35, 0.28, \"Random\", color=\"darkgray\", ha=\"center\")\n",
    "            else:\n",
    "                ax.text(8, 0.28, \"Random\", color=\"darkgray\", ha=\"center\")\n",
    "\n",
    "        if row_index == 0:\n",
    "            title = column_edit_methods[col_index]\n",
    "            ax.set_title(title, fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_title(\"\")\n",
    "\n",
    "        if col_index == 0:\n",
    "            ax.set_ylabel(row_labels[row_index], fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.set_xlabel(\"Sparsity\" if col_index < 3 else \"Bits\", fontsize=TITLE_FONT_SIZE)\n",
    "        else:\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "        if row_index == 2:\n",
    "            ax.legend(fontsize=LEGEND_FONT_SIZE, frameon=False, loc=\"upper center\", bbox_to_anchor=(0.5, -0.3), ncol=1)\n",
    "\n",
    "fig.subplots_adjust(wspace=WSPACE, hspace=WSPACE)\n",
    "plt.savefig(\"figures/main_results_editors_compression_with_ft.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Unlearning under Compression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_frame = data[((data[\"compression\"] == \"SparseGPT\") | (data[\"compression\"] == \"Wanda\")) & (data[\"unlearn\"] == \"RMU\")]\n",
    "pruning_frame[\"Avg WMDP\"] = (pruning_frame[\"wmdp_bio accuracy\"] + pruning_frame[\"wmdp_cyber accuracy\"]) / 2\n",
    "pruning_frame[\"order\"] = pruning_frame.apply(get_order_label, axis=1)\n",
    "pruning_frame = pruning_frame.sort_values(by=\"order\")\n",
    "\n",
    "quantization_frame = data[((data[\"compression\"] == \"GPTQ\") | (data[\"compression\"] == \"AWQ\")) & (data[\"unlearn\"] == \"RMU\")]\n",
    "quantization_frame[\"Avg WMDP\"] = (quantization_frame[\"wmdp_bio accuracy\"] + quantization_frame[\"wmdp_cyber accuracy\"]) / 2\n",
    "quantization_frame[\"order\"] = quantization_frame.apply(get_order_label, axis=1)\n",
    "quantization_frame = quantization_frame.sort_values(by=\"order\", ascending=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(5 * FIG_SIZE, FIG_SIZE))\n",
    "column_metrics = {\n",
    "    0: \"Avg WMDP\",\n",
    "    1: \"mmlu accuracy\",\n",
    "    2: \"Avg WMDP\",\n",
    "    3: \"mmlu accuracy\",\n",
    "}\n",
    "x_labels = {\n",
    "    \"sparsity_ratio\": \"Sparsity\",\n",
    "    \"wbits\": \"Bits\"\n",
    "}\n",
    "y_label = {\n",
    "    \"Avg WMDP\": r\"WMDP $\\downarrow$\",\n",
    "    \"mmlu accuracy\": r\"MMLU $\\uparrow$\"\n",
    "}\n",
    "\n",
    "# Custom legend elements\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=MARKER_SIZE, label='Editâ†’Compress'),\n",
    "    plt.Line2D([0], [0], marker='x', color='w', markerfacecolor='r', markersize=MARKER_SIZE, label='Compressâ†’Edit'),\n",
    "]\n",
    "\n",
    "for col_index, plotting_frame in enumerate([pruning_frame, pruning_frame, quantization_frame, quantization_frame]):\n",
    "    x_metric = \"sparsity_ratio\" if col_index < 2 else \"wbits\"\n",
    "    y_metric = \"Avg WMDP\" if col_index % 2 == 0 else \"mmlu accuracy\"\n",
    "    ax = axes[col_index]\n",
    "    ax.legend(title=\"Order\", loc=\"upper left\")\n",
    "    # sns.lineplot(data=plotting_frame, x=x_metric, y=y_metric, hue=\"order\", ax=ax, marker=\"o\", markersize=MARKER_SIZE, linewidth=LINE_WIDTH)\n",
    "\n",
    "    # Share area between compositions\n",
    "    orders_pairs = [(\"RMUâ†’SparseGPT\", \"SparseGPTâ†’RMU\"), (\"RMUâ†’Wanda\", \"Wandaâ†’RMU\"), (\"RMUâ†’GPTQ\", \"GPTQâ†’RMU\"), (\"RMUâ†’AWQ\", \"AWQâ†’RMU\")]\n",
    "    for order_pair in orders_pairs:\n",
    "        compression_method = [method for method in order_pair[0].split(\"â†’\") if method != \"RMU\"][0]\n",
    "        first_line = plotting_frame[plotting_frame[\"order\"] == order_pair[0]].sort_values(x_metric)\n",
    "        ax.plot(first_line[x_metric], first_line[y_metric], marker=\"o\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{order_pair[0]}\")\n",
    "        \n",
    "        second_line = plotting_frame[plotting_frame[\"order\"] == order_pair[1]].sort_values(x_metric)\n",
    "        ax.plot(second_line[x_metric], second_line[y_metric], markerfacecolor='none', marker=\"o\", ls=\"--\", markersize=MARKER_SIZE, color=colors[compression_method], label=f\"{order_pair[1]}\")\n",
    "        \n",
    "        ax.fill_between(\n",
    "            x=first_line[x_metric], y1=first_line[y_metric], y2=second_line[y_metric],\n",
    "            alpha=0.3,\n",
    "            color=colors[compression_method]\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(x_labels[x_metric], fontsize=TITLE_FONT_SIZE)\n",
    "    ax.set_ylabel(y_label[column_metrics[col_index]], fontsize=TITLE_FONT_SIZE)\n",
    "\n",
    "    ax.legend(frameon=False)\n",
    "    if col_index == 0:\n",
    "        ax.legend(loc=\"upper center\", bbox_to_anchor=(2.5, -0.25), ncol=4, frameon=False, fontsize=LEGEND_FONT_SIZE)\n",
    "    else:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    ax.axhline(y=0.25, color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "    if y_metric == \"mmlu accuracy\":\n",
    "        ax.set_ylim(0.2, 0.65)\n",
    "    else:\n",
    "        ax.set_ylim(0.2, 0.4)\n",
    "\n",
    "# add padding for labels\n",
    "fig.subplots_adjust(wspace=WSPACE + 0.15, hspace=WSPACE)\n",
    "plt.savefig(\"figures/unlearning_compression_1x4.pdf\", bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
