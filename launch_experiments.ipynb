{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Experiment Sweets\n",
    "\n",
    "This script grid searches over all listed experiment configurations and runs them sequentially. Optionally, runs that already exist in Weights and Biases can be skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Run Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['memit', 'rmu', 'wanda', 'awq']\n"
     ]
    }
   ],
   "source": [
    "models = [\"mistralai/Mistral-7B-Instruct-v0.3\"]\n",
    "\n",
    "# Interventions\n",
    "editing_interventions = [\"memit\"]\n",
    "unlearning_interventions = [\"rmu\"]\n",
    "pruning_interventions = [\"wanda\"]\n",
    "quantization_interventions = [\"awq\"]\n",
    "all_interventions = editing_interventions + unlearning_interventions + pruning_interventions + quantization_interventions\n",
    "print(all_interventions)\n",
    "\n",
    "# Intervention Settings\n",
    "pruning_levels = [0.25, 0.35, 0.45, 0.55, 0.65, 0.75]\n",
    "quant_levels = [2, 3, 4, 5, 6, 8]\n",
    "rmu_setting_overrides = {\n",
    "    \"rmu_alpha\": [1000, 1000],\n",
    "    \"rmu_layer_id\": 6,\n",
    "    \"rmu_max_num_batches\": 450,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('memit',),\n",
       " ('rmu',),\n",
       " ('wanda',),\n",
       " ('awq',),\n",
       " ('memit', 'rmu'),\n",
       " ('memit', 'wanda'),\n",
       " ('memit', 'awq'),\n",
       " ('rmu', 'wanda'),\n",
       " ('rmu', 'awq'),\n",
       " ('wanda', 'awq'),\n",
       " ('memit', 'rmu', 'wanda'),\n",
       " ('memit', 'rmu', 'awq'),\n",
       " ('memit', 'wanda', 'awq'),\n",
       " ('rmu', 'wanda', 'awq'),\n",
       " ('memit', 'rmu', 'wanda', 'awq')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_combinations(lst):\n",
    "    all_combs = []\n",
    "    for r in range(1, len(lst) + 1):\n",
    "        all_combs.extend(combinations(lst, r))\n",
    "\n",
    "    return all_combs\n",
    "\n",
    "\n",
    "all_intervention_combinations = get_all_combinations(all_interventions)\n",
    "all_intervention_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.25,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.35,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.45,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.55,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.65,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda',),\n",
       "  'sparsity_ratio': 0.75,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 2,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 3,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 4,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 5,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 6,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq',),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 8,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.25,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.35,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.45,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.55,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.65,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'wanda'),\n",
       "  'sparsity_ratio': 0.75,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.25,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.35,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.45,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.55,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.65,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'memit'),\n",
       "  'sparsity_ratio': 0.75,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 2,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 3,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 4,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 5,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 6,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'memit'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 8,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 2,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 3,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 4,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 5,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 6,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('memit', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 8,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.25,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.35,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.45,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.55,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.65,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('wanda', 'rmu'),\n",
       "  'sparsity_ratio': 0.75,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.25,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.35,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.45,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.55,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.65,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'wanda'),\n",
       "  'sparsity_ratio': 0.75,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 2,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 3,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 4,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 5,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 6,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('awq', 'rmu'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 8,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 2,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 3,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 4,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 5,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 6,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': ('rmu', 'awq'),\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 8,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'},\n",
       " {'interventions': [],\n",
       "  'sparsity_ratio': 0,\n",
       "  'wbits': 16,\n",
       "  'model_name': 'mistralai/Mistral-7B-Instruct-v0.3'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_interventions = 2\n",
    "run_configurations = []\n",
    "for intervention_combination in all_intervention_combinations:\n",
    "    if len(intervention_combination) > max_num_interventions:\n",
    "        continue\n",
    "\n",
    "    count_interventions = len(intervention_combination)\n",
    "    all_compression_interventions = pruning_interventions + quantization_interventions\n",
    "    is_double_compression = sum([technique in all_compression_interventions for technique in intervention_combination]) == 2\n",
    "    if is_double_compression:\n",
    "        continue\n",
    "\n",
    "    intervention_orderings = {intervention_combination, tuple(reversed(intervention_combination))}\n",
    "    for model_name in models:\n",
    "        for ordering in intervention_orderings:\n",
    "            if contains_pruning := any([technique in pruning_interventions for technique in ordering]):\n",
    "                for sparsity_ratio in pruning_levels:\n",
    "                    run_configurations.append({\"interventions\": ordering, \"sparsity_ratio\": sparsity_ratio, \"wbits\": 16, \"model_name\": model_name})\n",
    "            elif contains_quantization := any([technique in quantization_interventions for technique in ordering]):\n",
    "                for wbits in quant_levels:\n",
    "                    run_configurations.append({\"interventions\": ordering, \"sparsity_ratio\": 0, \"wbits\": wbits, \"model_name\": model_name})\n",
    "\n",
    "            default_sparsity_ratio = 0\n",
    "            default_wbits = 16\n",
    "\n",
    "\n",
    "# add one without interventions, just the model\n",
    "run_configurations.append({\"interventions\": [], \"sparsity_ratio\": 0, \"wbits\": 16, \"model_name\": model_name})\n",
    "\n",
    "run_configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Historical Run Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_previous_runs = False\n",
    "previous_runs = None\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.25 tag=WANDA25% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.35 tag=WANDA35% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.45 tag=WANDA45% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.55 tag=WANDA55% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.65 tag=WANDA65% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.75 tag=WANDA75% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=2 tag=AWQ2bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=3 tag=AWQ3bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=4 tag=AWQ4bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=5 tag=AWQ5bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=6 tag=AWQ6bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=awq wbits=8 tag=AWQ8bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.25 tag=MEMIT_WANDA25% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.35 tag=MEMIT_WANDA35% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.45 tag=MEMIT_WANDA45% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.55 tag=MEMIT_WANDA55% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.65 tag=MEMIT_WANDA65% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=wanda sparsity_ratio=0.75 tag=MEMIT_WANDA75% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.25 tag=WANDA25%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.35 tag=WANDA35%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.45 tag=WANDA45%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.55 tag=WANDA55%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.65 tag=WANDA65%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=wanda edit=memit sparsity_ratio=0.75 tag=WANDA75%_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=2 tag=AWQ2bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=3 tag=AWQ3bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=4 tag=AWQ4bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=5 tag=AWQ5bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=6 tag=AWQ6bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,edit] compression=awq edit=memit wbits=8 tag=AWQ8bit_MEMIT wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=2 tag=MEMIT_AWQ2bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=3 tag=MEMIT_AWQ3bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=4 tag=MEMIT_AWQ4bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=5 tag=MEMIT_AWQ5bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=6 tag=MEMIT_AWQ6bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[edit,compression] edit=memit compression=awq wbits=8 tag=MEMIT_AWQ8bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.25 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA25%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.35 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA35%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.45 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA45%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.55 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA55%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.65 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA65%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=wanda unlearning=rmu sparsity_ratio=0.75 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=WANDA75%_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.25 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA25% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.35 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA35% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.45 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA45% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.55 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA55% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.65 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA65% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=wanda sparsity_ratio=0.75 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_WANDA75% wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=2 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ2bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=3 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ3bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=4 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ4bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=5 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ5bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=6 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ6bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression,unlearning] compression=awq unlearning=rmu wbits=8 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=AWQ8bit_RMU wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=2 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ2bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=3 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ3bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=4 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ4bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=5 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ5bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=6 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ6bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[unlearning,compression] unlearning=rmu compression=awq wbits=8 rmu_alpha=[1000, 1000] rmu_layer_id=6 rmu_max_num_batches=450 tag=RMU_AWQ8bit wandb=online\n",
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[]   tag=NONE wandb=online\n",
      "Total number of runs: 61\n"
     ]
    }
   ],
   "source": [
    "def set_tag(experiment_row):\n",
    "    if experiment_row[\"interventions\"] in [None, np.nan]:\n",
    "        return \"NONE\"\n",
    "\n",
    "    intervention_categories = None\n",
    "    if isinstance(experiment_row[\"interventions\"], str):\n",
    "        intervention_categories = ast.literal_eval(experiment_row[\"interventions\"])\n",
    "    else:\n",
    "        intervention_categories = experiment_row[\"interventions\"]\n",
    "\n",
    "    interventions = []\n",
    "    for category in intervention_categories:\n",
    "        intervention = category.upper()\n",
    "        if intervention in [\"AWQ\", \"GPTQ\"]:\n",
    "            intervention += str(int(experiment_row[\"wbits\"])) + \"bit\"\n",
    "        if intervention in [\"WANDA\", \"SPARSEGPT\"]:\n",
    "            intervention += str(int(experiment_row[\"sparsity_ratio\"] * 100)) + \"%\"\n",
    "\n",
    "        interventions.append(intervention)\n",
    "\n",
    "    if len(interventions) == 0:\n",
    "        interventions.append(\"NONE\")\n",
    "\n",
    "    return \"_\".join(interventions)\n",
    "\n",
    "\n",
    "commands = []\n",
    "use_wandb = True\n",
    "is_slurm = False\n",
    "python_path = \"~/miniconda3/envs/lm-compose/bin/python\"\n",
    "\n",
    "run_commands = []\n",
    "for run_config in run_configurations:\n",
    "    intervention_type_map = {\"memit\": \"edit\", \"rmu\": \"unlearning\", \"wanda\": \"compression\", \"awq\": \"compression\"}\n",
    "    interventions_arg = [intervention_type_map[intervention] for intervention in run_config[\"interventions\"]]\n",
    "    interventions_arg_str = f\"interventions={interventions_arg}\".replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    category_args = [f\"{intervention_type_map[intervention]}={intervention}\" for intervention in run_config[\"interventions\"]]\n",
    "    category_args_str = \" \".join(category_args)\n",
    "\n",
    "    compress_args = []\n",
    "    for intervention in run_config[\"interventions\"]:\n",
    "        if intervention in pruning_interventions:\n",
    "            compress_args.append(f\"sparsity_ratio={run_config['sparsity_ratio']}\")\n",
    "        elif intervention in quantization_interventions:\n",
    "            compress_args.append(f\"wbits={run_config['wbits']}\")\n",
    "\n",
    "    command_prefix = \"sbatch run_exp.sh\" if is_slurm else f\"{python_path} -m lm_compose\"\n",
    "    command = command_prefix + f\" model_name={run_config['model_name']} {interventions_arg_str} {category_args_str} {' '.join(compress_args)}\"\n",
    "\n",
    "    involves_rmu = any([intervention in run_config[\"interventions\"] for intervention in unlearning_interventions])\n",
    "    if involves_rmu:\n",
    "        for key, value in rmu_setting_overrides.items():\n",
    "            command += f\" {key}={value}\"\n",
    "\n",
    "    tag = set_tag(run_config)\n",
    "    command += f\" tag={tag}\"\n",
    "\n",
    "    if use_wandb:\n",
    "        command += \" wandb=online\"\n",
    "\n",
    "    print(command)\n",
    "    run_commands.append(command)\n",
    "\n",
    "print(f\"Total number of runs: {len(run_configurations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Experiments:   0%|          | 0/61 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/miniconda3/envs/lm-compose/bin/python -m lm_compose model_name=mistralai/Mistral-7B-Instruct-v0.3 interventions=[compression] compression=wanda sparsity_ratio=0.25 tag=WANDA25% wandb=online\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: kyledevinobrien1 (dri-ice). Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.18.1 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.17.0\n",
      "wandb: Run data is saved locally in /sfs/weka/scratch/hua2bv/unlearning/composable-interventions/wandb/run-20240918_181749-o9ecfun7\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run WANDA25%-20240918_181748\n",
      "wandb: â­ï¸ View project at https://wandb.ai/dri-ice/Composable_Interventions\n",
      "wandb: ðŸš€ View run at https://wandb.ai/dri-ice/Composable_Interventions/runs/o9ecfun7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-18 18:17:53,075][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.01it/s]\n",
      "2024-09-18 18:17:56,225 - lm_compose.easyeditor.editors.editor - INFO - Instantiating model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-18 18:17:56,225][lm_compose.easyeditor.editors.editor][INFO] - Instantiating model\n",
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
      "[2024-09-18 18:17:56,549][lm_compose.easyeditor.editors.editor][INFO] - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 18:17:56,549 - lm_compose.easyeditor.editors.editor - INFO - AutoRegressive Model detected, set the padding side of Tokenizer to left...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune\n",
      "############# Begin intervention: compression #############\n",
      "prune\n",
      "use device  cuda:0\n",
      "pruning starts\n",
      "loading calibdation data\n",
      "Using c4\n"
     ]
    }
   ],
   "source": [
    "for command in tqdm(run_commands, desc=\"Running Experiments\"):\n",
    "    print(command)\n",
    "    # os.system(command)\n",
    "    # raise exception if command fails\n",
    "\n",
    "    code = os.system(command)\n",
    "    if code != 0:\n",
    "        raise Exception(f\"Command failed with code: {code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm-compose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
