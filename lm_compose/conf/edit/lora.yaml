alg_name: "LoRA"
lora_type: "adalora"
layers: []
num_steps: 70
lr: 5e-3 # 5e-3
weight_decay: 0.00
kl_factor: 0
rank: 8
lora_alpha: 32
lora_dropout: 0.0
norm_constraint: false
target_modules: ["q_proj", "v_proj"]  #["up_proj", "down_proj"] #["q_proj", "v_proj"]
dtype: torch.float
